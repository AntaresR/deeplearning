{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS231 NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning Neural Networks I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Convolution:** Elementwise multiplication and sum. Or we can mathematically write as a dot product of strecthed inputs and kernel weights\n",
    "\n",
    "\n",
    "* **Input (Convolution) Output Size**\n",
    "    - (N -F) / stride + 1\n",
    "    - stride: how many steps taken during image scan\n",
    "    - N: nxn image\n",
    "    - F: fxf kernel size\n",
    "    \n",
    "* **Subsampling** Most people use larger stride in convolutions to subsample rather than pooling, when using maxpooling common is to use 2x2 with 2 strides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W : dimension (square)\n",
    "# F : kernel size (square)\n",
    "# S : stride\n",
    "# P : padding\n",
    "\n",
    "def conv_calc(W, F, S, P): return (W - F + 2*P) / S + 1\n",
    "def pool_calc(W, F, S): return (W - F) / S + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Sigmoid: 1 / (1 + exp(-x))**\n",
    "\n",
    "Interpreted as a firing rate of neuron. (Good)\n",
    "\n",
    "Can kill the gradients for very small and large numbers (Bad)\n",
    "\n",
    "Output is not zero centered, weights will always increase or decrease (Bad)\n",
    "\n",
    "Exp is computationally expensive\n",
    "\n",
    "- **Tanh(x):**\n",
    "\n",
    "Same problems with sigmoid but zero centered\n",
    "\n",
    "- **Relu: max(0, x)**\n",
    "\n",
    "Does not saturate\n",
    "\n",
    "Very efficient\n",
    "\n",
    "Converges much faster\n",
    "\n",
    "Biologically more plausible than sigmoid\n",
    "\n",
    "Not zero centered\n",
    "\n",
    "When x < 0 zero gradient, never activate and update\n",
    "\n",
    "Dead RELUs\n",
    "\n",
    "- **Leaky Relu max(0.01x, x)**\n",
    "\n",
    "Does not saturate\n",
    "\n",
    "Efficient\n",
    "\n",
    "Converge fast\n",
    "\n",
    "Will not die, always activate\n",
    "\n",
    "\n",
    "- **Parametric Rectifier (PRelu) max(alphax, x)**\n",
    "\n",
    "alpha is learned with backprop\n",
    "\n",
    "- **Exponential Linear Units (ELU)**\n",
    "\n",
    "All benefits of ReLU\n",
    "\n",
    "Closer to zero mean outputs\n",
    "\n",
    "Negative saturation regime compared with Leaky Relu\n",
    "\n",
    "Adds robustness to noise\n",
    "\n",
    "Computation requires exp\n",
    "\n",
    "- **Maxout**\n",
    "\n",
    "Doubled parameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For images zero centering (Apply same mean from train to test)\n",
    "- Subtract the mean image (32, 32, 3) array - AlexNet\n",
    "- Subrtact per-channel mean (mean along each channel = 3 numbers) - VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- w = 0, all outputs will be the same and perfect symmetry over all weights (Bad)\n",
    "\n",
    "- All weights to be random gaussian - problem for deeper networks. As we move from first layer to deeper weights become 0.\n",
    "\n",
    "- Xavier Initialization (first try this)\n",
    "\n",
    "- More important as network gets deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a batch of activations at some layer, normalize by emprical mean and std\n",
    "- Usually inserted after FC and CONV layers\n",
    "\n",
    "- Normalization is important since data is scaled, and loss function becomes less sensitive to pertubrations\n",
    "\n",
    "- Always normalize after each layer outputs\n",
    "\n",
    "- Batch Normalization is an additional layer that normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) HYPERPARAMETER SEARCH \n",
    " \n",
    "- It's better than grid search, grid search makes one parameter sensitive to others\n",
    "\n",
    "- Coarse to find search, narrowing down your search space iteratively\n",
    "\n",
    "- lr, model sizes, regularization, but mostly problem dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training Neural Networks II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancier Optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Vanilla gradient descent\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "    weights += -step_size * weights_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with SGD\n",
    "\n",
    "- It may cause very slow progress along shallow dimension, jitter along steep direction (taco shell)\n",
    "\n",
    "- Loss function has high condition number, largest/smallest singular value of Hessian Matrix\n",
    "\n",
    "- As dimension gets larger the problem gets worse since condition numver becomes even larger\n",
    "\n",
    "- **Saddle Points**: SGD gets stucked since gradient zero. Happens very frequently as dimension increases. Optimization can't escape saddle points or will be at local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD + Momentum\n",
    "\n",
    "- Build up velocity as running mean or gradients\n",
    "- Rho gives \"friction\" typically 0.9 or 0.99\n",
    "- Helps with condition number problem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# the ball keep rolling with it's momentum even though it reaches a\n",
    "# flat surface\n",
    "rho = 0.9 #friction term\n",
    "vt = 0 #velocity\n",
    "lr = 0.01\n",
    "while True:\n",
    "    dx = compute_gradient(loss_func, weights, data)\n",
    "    vt = rho * vt + dx\n",
    "    x +=  -lr * vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum \n",
    "\n",
    "- Take a step by velocity\n",
    "- Compute gradient at that point\n",
    "- Take a step in gradient direction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dx = compute_gradient(loss_func, weights, data)\n",
    "old_v = v\n",
    "v = rho * old_v - lr * dx\n",
    "x += v + rho * (v - old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "- Keep running sum of gradients\n",
    "- In update divide by this term\n",
    "- With very high condition number adjusts the gradient\n",
    "- Its always shrinks, RMSProp is better in this context"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "grad_squared = 0\n",
    "while True:\n",
    "    dx = compute_gradient(loss_func, weights, data)\n",
    "    grad_squared += dx*dx\n",
    "    x -= lr * dx / (np.sqrt(grad_squared) + 1e-7) #avoid zero division\n",
    "    \n",
    "### RMSProp\n",
    "decay_rate = 0.9\n",
    "grad_squared = 0\n",
    "while True:\n",
    "    dx = compute_gradient(loss_func, weights, data)\n",
    "    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx\n",
    "    x -= lr *dx / (np.sqrt(grad_squared) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "- Sort of like RMSProp with momentum\n",
    "- beta1 = 0.9\n",
    "- beta2 = 0.99\n",
    "- lr = 1e-3 - 1e-4 good start"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "first_moment = 0\n",
    "second_moment = 0 # problem of taking a very big step at the beginning\n",
    "for t in range(num_iterations):\n",
    "    dx = compute_gradient(loss_func, weigths, data)\n",
    "    first_moment = beta1 * first_moment + (1 - beta1) * dx #Momentum\n",
    "    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx\n",
    "    #x -= lr * first_moment / (np.sqrt(second_moment) + 1e-7) # AdaGrad/RMSProp - Problem\n",
    "    #correcting for taking a very big step at the beginning\n",
    "    first_unbias = first_moment / (1 - beta**t)\n",
    "    second_unbias = second_moment / (1 - beta**t)\n",
    "    x -= lr * first_unbias / (np.sqrt(second_unbias) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to keep the same learning rate we can change learning rate during training. Learning rate decay (common with SGD Momentum).\n",
    "\n",
    "First choose a good learning rate, check your loss curve then decide where you might need learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-Order Optimization Newton parameter update**\n",
    "\n",
    "Bad for deeplearning inverting is O(N**3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBFGS\n",
    "\n",
    "- If you can afford to do full batch updates then try out (and discard sources of noise since this method is not prone to probabilistic framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CREATING BETTER MODELS\n",
    "\n",
    "#### A) MODEL ENSEMBLES\n",
    "\n",
    "- Train independent models\n",
    "- At test time average their results (+%2 performance)\n",
    "\n",
    "#### B) SNAPSHOT ENSEMBLE\n",
    "\n",
    "- Cyclic Learning Rate Schedules\n",
    "- Taking snapshots at different minimas and averaging them\n",
    "- Training the model once \n",
    "\n",
    "#### C) POLYAK AVERAGING\n",
    "\n",
    "- Keep moving average of the parameter vector and use this at test time\n",
    "- And use this vector at test time\n",
    "\n",
    "#### D) REGULARIZATION\n",
    "\n",
    "- L2 \n",
    "- L1\n",
    "- Elastic Net\n",
    "\n",
    "- For neural networks:\n",
    "\n",
    "#### DROPOUT\n",
    "\n",
    "- Dropoout (cancel some activations with Pr = p) - don't effect next layer\n",
    "- More common in FC layers but may also be used in CONV\n",
    "- Dropout helps with co-adaptation of features, generalizes\n",
    "- It's like a gigantic ensemble within a single model\n",
    "- At test time, multiply by dropout probability\n",
    "\n",
    "#### ADD NOISE\n",
    "\n",
    "- Training: Add some randomness\n",
    "- Test: Average out randomness\n",
    "\n",
    "#### BATCH NORMALIZATION\n",
    "\n",
    "- Training: Normalize using stats from random minibatches\n",
    "- Testing: Use fixed stats to normalize\n",
    "\n",
    "#### DATA AUGMENTATION\n",
    "\n",
    "- Traning: Sample random Flips, crops, ...\n",
    "- Test: Average 5 augmented predictions\n",
    "- You can go crazy with this, mixed combinations without changing the label\n",
    "\n",
    "#### DROP CONNECT\n",
    "\n",
    "- Rather than zero outing activations this time we are zero outing weights from weight matrix\n",
    "\n",
    "#### FRACTIONAL MAXPOOLING\n",
    "\n",
    "- Training: Add random noise\n",
    "- Test: Marginalize over the noise\n",
    "- Every pooling layer will have random regions\n",
    "\n",
    "#### STOCHASTIC DEPTH\n",
    "\n",
    "- Training: Randomly dropping layers\n",
    "- Test: Use whole network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Polyak Averaging\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    x += - lr * dx\n",
    "    x_test = 0.995*x_test + 0.005*x #use for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Regularization with dropout\n",
    "def train_step(X):\n",
    "    '''X contains data'''\n",
    "    \n",
    "    # Forward pass example for 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W1, X) + b1) # first activation with ReLU\n",
    "    U1 = (np.random.rand(*H1.shape) < p) / p # first drop out\n",
    "    H1 *= U1 # drop\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "    U2 = (np.random.rand(*H2.shape) < p) / p\n",
    "    H2 *= U2\n",
    "    out = np.dot(W3, H2) + b3\n",
    "    \n",
    "def predict(X):\n",
    "    # ensembled forward pass\n",
    "    H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "    out = np.dot(W3, H2) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take pretrained network\n",
    "- Change last FC layer (or n last FC layers)\n",
    "- Good with small data\n",
    "- If you have enough data you can fine tune the whole FC part of the network after updated the last layer (freeze CONV layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GUIDE TO DEAL WITH CASES BY PRETRAINED NETS:**\n",
    "\n",
    "- **very little data & very similar dataset (to ImageNet)**\n",
    "    - Use linear classifier on top layer\n",
    "    - Basically just update the last FC layer \n",
    "- **lot of data & very similar dataset**\n",
    "    - Fine tune a few layers (FC with convs freezed)\n",
    "- **lot of data & very different dataset**\n",
    "    - fine tune a larger number of layers after update (may include conv parts - lrs with differential learning rates)\n",
    "- **very little data & very different dataset**\n",
    "    - Need to be more creative since last layer won't help with this case. Re-initilialize more layers and experiment. (TC scans, MRIs)\n",
    "    \n",
    "    \n",
    "Download a pretrained model close to your model,  update last layer or fine tune.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4) FASTAI TRICKS\n",
    "\n",
    "**1)** lr_find(): Increases lr until loss increases so that we can find the optimal learning rate that will allow us to converge fast enough while not jumping over and passing a minima.\n",
    "\n",
    "**2)** Stochastic Gradient Descent with Restarts(SGDR): Decreases lr gradually to slowly converge as we are getting closer to a minima then increases back to starting level to escape local minimas in order to find a better minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
