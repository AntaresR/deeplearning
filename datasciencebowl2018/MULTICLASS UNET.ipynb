{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.utils import *\n",
    "from dataset.dataset import *\n",
    "from dataset.transform import *\n",
    "from model.loss import *\n",
    "from model.unet import UNet256_3x3\n",
    "from training.classifier import NucleiClassifier\n",
    "from model.eval import *\n",
    "from dataset.submission import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage.morphology import label\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create Random (Stratified) Train and Validation Set - RUN AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classes = pd.read_csv('classes.csv')\n",
    "train_dirs, valid_dirs = get_stratified_valid_dirs(classes, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main_path = '../data/ds_bowl_2018/'\n",
    "full_path = '../data/ds_bowl_2018/full_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 606 training and 64 validation data\n"
     ]
    }
   ],
   "source": [
    "create_validation_dirs(main_path, full_path, None, train_dirs, valid_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreground</th>\n",
       "      <th>background</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">purple</th>\n",
       "      <th>purple</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  is_train\n",
       "foreground background                    \n",
       "purple     purple             1         1\n",
       "           white              3         3\n",
       "           yellow             8         8\n",
       "white      black             53        53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test distribution\n",
    "classes[~classes.is_train].groupby(['foreground', 'background']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreground</th>\n",
       "      <th>background</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">purple</th>\n",
       "      <th>purple</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  is_train\n",
       "foreground background                    \n",
       "purple     purple             2         2\n",
       "           white              4         4\n",
       "white      black             58        58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distributions seem similar to test\n",
    "classes[classes.filename.isin([f + '.png' for f in valid_dirs])].groupby(['foreground', 'background']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Prep\n",
    "\n",
    "In multiclass UNET we will not be outputing a single channel like we did for binary classification background or mask but rather this time we will have a 3 channel output. Each channel will correspond to probability distributions for background, mask and overlap. We will be using multiclass BCE Loss.\n",
    "\n",
    "Output Channels:\n",
    "\n",
    "- Background\n",
    "- Foreground\n",
    "- Overlap\n",
    "\n",
    "\n",
    "If there are 3 unique pixel intensities\n",
    "\n",
    "- 30 : Background\n",
    "- 110 : Nuclei\n",
    "- 215 : Overlap Boundary\n",
    "\n",
    "If there are 2 unique pixel intensities\n",
    "\n",
    "- 30 : Background\n",
    "- 215 : Nuclei\n",
    "\n",
    "w_background, w_nuclei, w_overlap = (1.1551767249306626, 7.544967099214484, 557.753645718466)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/ds_bowl_2018/train/'\n",
    "valid_path = '../data/ds_bowl_2018/valid/'\n",
    "dummy_path = '../data/ds_bowl_2018/dummy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirs = list_directory(train_path)\n",
    "valid_dirs = list_directory(valid_path)\n",
    "dummy_dirs = list_directory(dummy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/ds_bowl_2018/train/58406ed8ef944831c413c3424dc2b07e59aef13eb1ff16acbb3402b38b5de0bd/images/',\n",
       " '../data/ds_bowl_2018/train/58406ed8ef944831c413c3424dc2b07e59aef13eb1ff16acbb3402b38b5de0bd/one_mask.png/',\n",
       " '../data/ds_bowl_2018/train/58406ed8ef944831c413c3424dc2b07e59aef13eb1ff16acbb3402b38b5de0bd/mclass_one_mask.png/',\n",
       " '../data/ds_bowl_2018/train/58406ed8ef944831c413c3424dc2b07e59aef13eb1ff16acbb3402b38b5de0bd/masks/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_directory(train_dirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606, 64, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dirs), len(valid_dirs), len(dummy_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_mask(mask):\n",
    "    if len(np.unique(mask)) == 2:\n",
    "        back_channel = (mask == 30)*1 # background\n",
    "        nuclei_channel = (mask == 215)*1 # nuclei\n",
    "        overlap_channel = np.zeros_like(mask) # overlap - missing so all 0s\n",
    "    else:\n",
    "        back_channel = (mask == 30)*1 # background\n",
    "        nuclei_channel = (mask == 110)*1 # nuclei\n",
    "        overlap_channel = (mask == 215)*1 # overlap\n",
    "    # stack depth-wise\n",
    "    multiclass_mask = np.dstack([back_channel, nuclei_channel, overlap_channel])\n",
    "    return multiclass_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augment(image, mask, index, sz = 256):           \n",
    "    # get 3d mask\n",
    "    mask3d = get_3d_mask(mask).astype(np.uint8) \n",
    "    # resize image, mask for Unet256\n",
    "    image = fix_resize_transform(image, sz, sz)\n",
    "    mask3d = fix_resize_transform(mask3d, sz, sz) \n",
    "    # get dimensions right for pytorch\n",
    "    image = (image.transpose((2,0,1))) / 255 \n",
    "    mask3d = (mask3d.transpose((2,0,1)))\n",
    "    return image.astype(np.float64), mask3d.astype(np.float64), index\n",
    "\n",
    "def valid_augment(image, mask, index, sz = 256):        \n",
    "    # get 3d mask\n",
    "    mask3d = get_3d_mask(mask).astype(np.uint8) \n",
    "    # resize image, mask for Unet256\n",
    "    image = fix_resize_transform(image, sz, sz)\n",
    "    mask3d = fix_resize_transform(mask3d, sz, sz) \n",
    "    # get dimensions right for pytorch\n",
    "    image = (image.transpose((2,0,1))) / 255 \n",
    "    mask3d = (mask3d.transpose((2,0,1)))\n",
    "    return image.astype(np.float64), mask3d.astype(np.float64), index\n",
    "\n",
    "\n",
    "def test_augment(image, mask, index, sz = 256):\n",
    "    # resize image for Unet256\n",
    "    image = fix_resize_transform(image, sz, sz)\n",
    "    # normalize pixel intensities\n",
    "    image = (image.transpose((2,0,1))) / 255 \n",
    "    return image.astype(np.float64), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummyloader\n",
    "dummy_ds = NucleiDataset('../data/ds_bowl_2018/dummy/', transform=train_augment, mode='train', mask_file='/mclass_one_mask.png')\n",
    "dummy_dl = DataLoader(dummy_ds, batch_size=1, shuffle=False)\n",
    "# create trainloader\n",
    "trn_ds = NucleiDataset('../data/ds_bowl_2018/train/', transform=train_augment, mode='train', mask_file='/mclass_one_mask.png')\n",
    "trn_dl = DataLoader(trn_ds, batch_size=8, shuffle=True)\n",
    "# create validloader.png\n",
    "val_ds = NucleiDataset('../data/ds_bowl_2018/valid/', transform=valid_augment, mode='valid', mask_file='/mclass_one_mask.png')\n",
    "val_dl = DataLoader(val_ds, batch_size=8, shuffle=False)\n",
    "# create testloader\n",
    "test_ds = NucleiDataset('../data/ds_bowl_2018/test/', transform=test_augment, mode='test')\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_BCELoss2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Weights for a single sample which is repeated along the batch\n",
    "    Inputs:\n",
    "        weight: weigth tensor for a single sample\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None):\n",
    "        super(Weighted_BCELoss2d, self).__init__()\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        # contruct bce loss\n",
    "        batch_size = targets.size(0)\n",
    "        weights = self.weight.repeat(batch_size)\n",
    "        bce_loss = nn.BCELoss(weights)\n",
    "        # calc loss\n",
    "        probs        = F.sigmoid(logits)\n",
    "        probs_flat   = probs.view (-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        return bce_loss(probs_flat, targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_background, w_nuclei, w_overlap = (1.155, 7.544, 557.753)\n",
    "weights = torch.cat([torch.ones(256*256)*w_background, \n",
    "                     torch.ones(256*256)*w_nuclei, torch.ones(256*256)*w_overlap]).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training definition\n",
    "net = UNet256_3x3(in_shape=(3, 256, 256), num_classes=3).double()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.01)\n",
    "# weighted BCELoss\n",
    "crit = Weighted_BCELoss2d(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init classifier\n",
    "classifier = NucleiClassifier(net, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186\n",
      "Training : [0.1159],Validation : [0.3826]\n",
      "Epoch: 187\n",
      "Training : [0.1147],Validation : [0.3467]\n",
      "Epoch: 188\n",
      "Training : [0.1242],Validation : [0.3864]\n",
      "Epoch: 189\n",
      "Training : [0.1179],Validation : [0.374]\n",
      "Epoch: 190\n",
      "Training : [0.1115],Validation : [0.3399]\n",
      "Epoch: 191\n",
      "Training : [0.1119],Validation : [0.3592]\n",
      "Epoch: 192\n",
      "Training : [0.1078],Validation : [0.363]\n",
      "Epoch: 193\n",
      "Training : [0.106],Validation : [0.3583]\n",
      "Epoch: 194\n",
      "Training : [0.1172],Validation : [0.3825]\n",
      "Epoch: 195\n",
      "Training : [0.1063],Validation : [0.3583]\n",
      "Epoch: 196\n",
      "Training : [0.1015],Validation : [0.3751]\n",
      "Epoch: 197\n",
      "Training : [0.1014],Validation : [0.3934]\n",
      "Epoch: 198\n",
      "Training : [0.1077],Validation : [0.3704]\n",
      "Epoch: 199\n",
      "Training : [0.1177],Validation : [0.418]\n",
      "Epoch: 200\n",
      "Training : [0.1158],Validation : [0.3962]\n",
      "Epoch: 201\n",
      "Training : [0.1082],Validation : [0.4368]\n",
      "Epoch: 202\n",
      "Training : [0.1203],Validation : [0.3862]\n",
      "Epoch: 203\n",
      "Training : [0.1034],Validation : [0.3871]\n",
      "Epoch: 204\n",
      "Training : [0.0998],Validation : [0.3832]\n",
      "Epoch: 205\n",
      "Training : [0.0988],Validation : [0.4044]\n"
     ]
    }
   ],
   "source": [
    "# train for 10 epochs\n",
    "classifier.train(train_loader=dummy_dl, valid_loader=dummy_dl, optimizer=optimizer,crit=crit, epochs=20, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = iter(dummy_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = V(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class index by max logit\n",
    "_, out_mask = torch.max(out[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_mask.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_with_sz(out_mask.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
