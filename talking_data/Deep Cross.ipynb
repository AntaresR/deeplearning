{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:https://arxiv.org/pdf/1708.05123.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', 100)\n",
    "pd.set_option('display.max_row', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../fastai/')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "from xgb_learn.learner import *\n",
    "from xgb_learn.dataset import *\n",
    "from eda.simple import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "from fastai.column_data import *\n",
    "from fastai.structured import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Outer Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5 # batch size\n",
    "p = 10 # vec dimension\n",
    "u0 = torch.ones(bs, p)\n",
    "\n",
    "u1 = torch.bmm(V(u0.unsqueeze(2)), V(u0.unsqueeze(1)))\n",
    "\n",
    "w1 = nn.Linear(p, 1).cuda()\n",
    "\n",
    "u1.size()\n",
    "\n",
    "type(w1(V(u1))), type(u0.unsqueeze(2))\n",
    "\n",
    "torch.add(w1(V(u1)), V(u0.unsqueeze(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_data = pd.read_feather(\"../../../data/talking/train_small_data.feather\")\n",
    "val_small_data = pd.read_feather(\"../../../data/talking/val_small_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take samples from train small data and val small data\n",
    "sample_ratio =  0.03\n",
    "np.random.seed(7)\n",
    "train_idx = np.random.choice(train_small_data.index, size=int(len(train_small_data)*sample_ratio), replace=False)\n",
    "val_idx = np.random.choice(val_small_data.index, size=int(len(val_small_data)*sample_ratio), replace=False)\n",
    "\n",
    "# create sample data\n",
    "train_small_sample = train_small_data.iloc[train_idx].reset_index(drop=True, inplace=False)\n",
    "val_small_sample = val_small_data.iloc[val_idx].reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_feather(\"../../../data/talking/nn_small_sample_full_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nrows, val_nrows, test_nrows = (1916846, 687869, 18790469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = full_data[:train_nrows]\n",
    "val_df = full_data[train_nrows:train_nrows+val_nrows]\n",
    "test_df = full_data[-test_nrows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_y = train_small_sample.is_attributed\n",
    "val_y = val_small_sample.is_attributed # from data frames expects index 0,1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.reset_index(drop=True, inplace=True)\n",
    "val_y.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['ip', 'app', 'device', 'os', 'channel',\n",
    "'click_timeDay', 'click_timeHour']\n",
    "\n",
    "# get cat sizes\n",
    "cat_sz = [(c, len(full_data[c].unique())) for c in cats]\n",
    "\n",
    "# create embedding sizes\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
    "\n",
    "# nconts\n",
    "n_conts = len(full_data.columns) - len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Changed Validation Batch Size to be large enough for roc auc calculation\n",
    "\n",
    "class ColumnarModelData(ModelData):\n",
    "    def __init__(self, path, trn_ds, val_ds, bs, test_ds=None, shuffle=True):\n",
    "        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n",
    "        super().__init__(path, DataLoader(trn_ds, bs, shuffle=shuffle, num_workers=1),\n",
    "            DataLoader(val_ds, bs*50, shuffle=False, num_workers=1), test_dl) # increased validation batch size\n",
    "\n",
    "    @classmethod\n",
    "    def from_arrays(cls, path, val_idxs, xs, y, is_reg=True, is_multi=False, bs=64, test_xs=None, shuffle=True):\n",
    "        ((val_xs, trn_xs), (val_y, trn_y)) = split_by_idx(val_idxs, xs, y)\n",
    "        test_ds = PassthruDataset(*(test_xs.T), [0] * len(test_xs), is_reg=is_reg, is_multi=is_multi) if test_xs is not None else None\n",
    "        return cls(path, PassthruDataset(*(trn_xs.T), trn_y, is_reg=is_reg, is_multi=is_multi),\n",
    "                   PassthruDataset(*(val_xs.T), val_y, is_reg=is_reg, is_multi=is_multi),\n",
    "                   bs=bs, shuffle=shuffle, test_ds=test_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frames(cls, path, trn_df, val_df, trn_y, val_y, cat_flds, bs, is_reg, is_multi, test_df=None):\n",
    "        test_ds = ColumnarDataset.from_data_frame(test_df, cat_flds, None, is_reg, is_multi) if test_df is not None else None\n",
    "        return cls(path, ColumnarDataset.from_data_frame(trn_df, cat_flds, trn_y, is_reg, is_multi),\n",
    "                    ColumnarDataset.from_data_frame(val_df, cat_flds, val_y, is_reg, is_multi), bs, test_ds=test_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, path, val_idxs, df, y, cat_flds, bs, is_reg=True, is_multi=False, test_df=None):\n",
    "        ((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idxs, df, y)\n",
    "        return cls.from_data_frames(path, trn_df, val_df, trn_y, val_y, cat_flds, bs, is_reg, is_multi, test_df=test_df)\n",
    "\n",
    "    def get_learner(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                    y_range=None, use_bn=False, **kwargs):\n",
    "        model = MixedInputModel(emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn, self.is_reg, self.is_multi)\n",
    "        return StructuredLearner(self, StructuredModel(to_gpu(model)), opt_fn=optim.Adam, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossDenseNN(nn.Module):\n",
    "    #https://arxiv.org/pdf/1708.05123.pdf\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, cross_depth=6,\n",
    "                 y_range=None, use_bn=False, is_reg=True, is_multi=False):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont=n_emb, n_cont\n",
    "        self.cross_depth = cross_depth\n",
    "        \n",
    "        szs = [n_emb+n_cont] + szs\n",
    "        self.szs = szs\n",
    "        self.lins = nn.ModuleList([\n",
    "            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]])\n",
    "        for o in self.lins: kaiming_normal(o.weight.data)\n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.use_bn,self.y_range = use_bn,y_range\n",
    "        self.is_reg = is_reg\n",
    "        self.is_multi = is_multi\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if not hasattr(self, \"lins2\"):\n",
    "            bs, _ = x_cat.size()\n",
    "            self.lins2 = nn.ModuleList([nn.Linear(self.n_emb + self.n_cont, 1).cuda()\n",
    "                          for i in range(self.ross_depth)])\n",
    "            self.l_out = nn.Linear((self.n_emb + self.n_cont) + self.szs[-1], 2).cuda()\n",
    "            \n",
    "        \n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "            \n",
    "        # DNN    \n",
    "        x_dnn = x\n",
    "        for l,d,b in zip(self.lins, self.drops, self.bns):\n",
    "            x_dnn = F.relu(l(x_dnn))\n",
    "            if self.use_bn: x = b(x)\n",
    "            x_dnn = d(x_dnn) \n",
    "        \n",
    "        # CROSS NN\n",
    "        xl = x\n",
    "        x0 = x\n",
    "        for l in self.lins2:\n",
    "            xl = l(torch.bmm(x0.unsqueeze(2), xl.unsqueeze(1))).squeeze() + xl # bs x p \n",
    "            \n",
    "            \n",
    "        return F.log_softmax(self.l_out(torch.cat([x_dnn, xl], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = CrossDenseNN(emb_szs,\n",
    "                    n_cont=n_conts,\n",
    "                    emb_drop=0.5,\n",
    "                    out_sz=2,\n",
    "                    szs=[500,500],\n",
    "                    drops=[0.5, 0.5],\n",
    "                    is_reg=False,\n",
    "                    is_multi=False).cuda()\n",
    "\n",
    "\n",
    "bm = BasicModel(model, 'binary_classifier')\n",
    "\n",
    "# initialize model data\n",
    "md = ColumnarModelData.from_data_frames('/tmp',\n",
    "                                        trn_df,\n",
    "                                        val_df,\n",
    "                                        trn_y,\n",
    "                                        val_y,\n",
    "                                        cats,\n",
    "                                        128, False, False, test_df=test_df)\n",
    "\n",
    "# initialize learner\n",
    "learn = StructuredLearner(md, bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9965291fb53c471eb811f63fdd9bf694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 13396/14976 [08:34<01:00, 26.04it/s, loss=0.0164] "
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [accuracy, auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline accuracy\n",
    "all_zeros = 1 - val_y.mean()\n",
    "all_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc metric\n",
    "def auc(preds, targs):\n",
    "    score = roc_auc_score(to_np(targs), to_np(preds)[:, 1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 4e-3\n",
    "learn.fit(lr, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2,y=next(iter(md.val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = learn.model(V(x1), V(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
